# Auto generated file using CookiCutter
import json
import os
from datetime import datetime
from airflow.models import Variable
from airflow.kubernetes.secret import Secret
from airflow.operators.empty import EmptyOperator
from airflow.providers.google.cloud.operators.kubernetes_engine import GKEStartPodOperator
from airflow.utils.task_group import TaskGroup
from data_algorithm.feature_store.include.abstractions.dag_parameters import PROJECT_ID_ENV_VARS
from data_algorithm.feature_store.include.teams.responders import get_team_responders
from data_algorithm.feature_store.plugins.sensors.sql_sensor_dbt import SourceTable
from data_algorithm.feature_store.plugins.operators.fs_materialize import OnlineStoreMaterialization
from kubernetes.client import models as k8s
from talabat_airflow_utils.operators.bigquery_sensor import TalabatBigQueryCountSensorOperator
from talabat_airflow_utils.operators.dag_trigger import TalabatTriggerDagRunOperator
from tdata_utils.airflow.talabat_dag import TalabatDAG
from kubernetes.client.models import V1EnvVar, V1ResourceRequirements

default_args = {
    "owner": " ,".join(get_team_responders("MlOps", name=True) + get_team_responders("DM", name=True)),
    "email": " ,".join(get_team_responders("MlOps", email=True) + get_team_responders("DM", email=True)),
    "start_date": datetime(2024, 6, 1),
    "depends_on_past": False,
    "catchup": False,
    "use_legacy_sql": False,
    "bigquery_conn_id": "bigquery_default",
}


default_node_pool_name = "algo-generic-node-pool"
default_commands = ["python", "main.py"]
cluster_name = Variable.get("algo_cluster_name")
project_id = os.environ["GCP_PROJECT"]
branch = "main" if project_id == "tlb-data-prod" else "develop"
dag_id_str = "{{ cookiecutter.dag_id }}"

dag_doc = (
    f"\n\n**Auto Gernerated DAG**"
    f"\n\nThis DAG is auto-generated for MlFlow:"
    f"\n\n - **Project**: {{ cookiecutter.project_name }} "
    f"\n\n - **Model**: {{ cookiecutter.model_name }} "
    f"\n\n - **Owner**: {{ cookiecutter.owner }}."
    f"\n\nThis DAG is generated using the following user input YAML file: [{{ cookiecutter.project_name }}/{{ cookiecutter.model_name }}](https://github.com/talabat-dhme/data-ml-pipelines/tree/{branch}/projects/{{ cookiecutter.project_name }}/{{ cookiecutter.model_name }}/schedule.yaml)"
)


with TalabatDAG(
        dag_id="{{ cookiecutter.dag_id }}",
        schedule_interval="{{ cookiecutter.schedule_interval }}",
        description="MLFlow dag autogenerated for Project: {{ cookiecutter.project_name }} and Model: {{ cookiecutter.model_name }} Owner: {{ cookiecutter.owner }}.",
        tags=["mlflow", "{{ cookiecutter.project_name  }}", "{{ cookiecutter.model_name }}",
              "{{ cookiecutter.owner }}"],
        default_args=default_args,
        doc_md=dag_doc,
        concurrency=12,
) as dag:
    start = EmptyOperator(task_id='start')
    end = EmptyOperator(task_id='end')
    all_groups_end = EmptyOperator(task_id='all_groups_end')
    tasks_group_dict = {{cookiecutter.tasks_group}}
    tasks_group_dict = tasks_group_dict['DAG_TASKS_GROUP']

    for task_group in tasks_group_dict:
        with TaskGroup(group_id=task_group.get("task_group_name")) as main_task_group:

            # BigQuery Sensors Tasks
            previous_sensor_task = start
            if task_group.get("sql_sensors", []):
                with TaskGroup(group_id='sql_sensor_{{ cookiecutter.dag_id }}') as bigquery_sensor_tasks:
                    sensor_tasks = []
                    for sql_sensor in task_group.get("sql_sensors", []):
                        task_id = sql_sensor.get("task_id")
                        sensor_enabled = sql_sensor.get("enable", None)
                        table_name = sql_sensor.get("table_name", None)
                        dataset_name = sql_sensor.get("dataset_name", None)
                        sensor_properties = sql_sensor.get("sensor_properties", None)
                        if sensor_enabled:
                            source_table_attributes = {
                                "sensor_priority": sensor_enabled,
                                "table_name": table_name,
                                "dataset_name": dataset_name,
                                "sensor_properties": sensor_properties,
                            }
                            print(f"DAG[{dag_id_str}] Calling sensor with attributes: {json.dumps(source_table_attributes)}" )
                            source = SourceTable(**source_table_attributes)
                            current_sensor_task = TalabatBigQueryCountSensorOperator(
                                task_id=task_id,
                                poke_interval=60 * 5,
                                timeout=10 * 60 * 60,
                                sql=source.get_sql_sensor()
                            )

                            sensor_tasks.append(current_sensor_task)

                        else:
                            print(f"Sensor {table_name} is not enabled")
                start >> bigquery_sensor_tasks
                previous_sensor_task = bigquery_sensor_tasks

            # Trigger DAG Tasks
            previous_trigger_dag_task = previous_sensor_task
            if task_group.get("trigger_dags", []):
                with TaskGroup(group_id='trigger_dags_{{ cookiecutter.dag_id }}') as trigger_dags_task_group:
                    for dag_to_trigger in task_group.get('trigger_dags', []):
                        trigger_dag_id = dag_to_trigger.get("trigger_dag_id", None)
                        if trigger_dag_id and trigger_dag_id != "":
                            current_trigger_dag_task = TalabatTriggerDagRunOperator(
                                task_id=f"trigger_dag_{trigger_dag_id}",
                                trigger_dag_id=trigger_dag_id,
                                execution_date="{% raw %}{{ execution_date }}{% endraw %}",
                                wait_for_completion=True
                            )
                        else:
                            print(f"trigger_dag_id can't be empty or None")

                        previous_trigger_dag_task >> current_trigger_dag_task
                        previous_trigger_dag_task = current_trigger_dag_task

            # GKE POD Tasks
            previous_gke_task = previous_trigger_dag_task
            if task_group.get("gke_tasks", []):
                with TaskGroup(group_id='gke_tasks_{{ cookiecutter.dag_id }}') as gke_task_group:
                    for gke_task in task_group.get('gke_tasks', []):

                        project_model = "{{ cookiecutter.project_name  }}/{{ cookiecutter.model_name }}"
                        script_arguments = gke_task.get("commands", default_commands)
                        # Define the shared memory volume
                        shared_memory_volume = k8s.V1Volume(
                        name = "dshm", empty_dir = k8s.V1EmptyDirVolumeSource(medium="Memory")
                        )

                        # Mount the shared memory volume in the container
                        volume_mount = k8s.V1VolumeMount(name="dshm", mount_path="/dev/shm")

                        # Get resource requirements from gke_task config
                        container_resources_config = gke_task.get('container_resources', None)
                        gke_container_resources = None
                        if container_resources_config:
                            gke_container_resources = V1ResourceRequirements(
                                requests=container_resources_config.get('requests', None),
                                limits=container_resources_config.get('limits', None)
                            )

                        current_gke_task = GKEStartPodOperator(
                            task_id = gke_task.get('task_id'),
                            project_id = project_id,
                            location = "europe-west2",
                            cluster_name = cluster_name,
                            startup_timeout_seconds = 180,
                            is_delete_operator_pod = False,
                            namespace = "default",
                            retries = 1,
                            secrets = [Secret(
                                deploy_type="volume",
                                deploy_target="/var/secrets/google",
                                secret="service-account",
                                key="service-account.json",
                            )],
                            env_vars = PROJECT_ID_ENV_VARS,
                            image = f"gcr.io/{project_id}/algo-master-image:latest",
                            cmds = ["/usr/src/app/entrypoint.sh"],
                            arguments = [project_model] + script_arguments,
                            name = "gke_pod_task_{{ cookiecutter.model_name }}",
                            affinity = {
                                "nodeAffinity": {
                                    "requiredDuringSchedulingIgnoredDuringExecution": {
                                        "nodeSelectorTerms": [
                                            {
                                                "matchExpressions": [
                                                    {
                                                        "key": "cloud.google.com/gke-nodepool",
                                                        "operator": "In",
                                                        "values": [Variable.get(gke_task.get("node_pool", default_node_pool_name))],
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                }
                            },
                            volumes = [shared_memory_volume],
                            volume_mounts = [volume_mount],
                            container_resources=gke_container_resources  # Will be None if not specified in schedule.yaml
                        )

                        previous_gke_task >> current_gke_task
                        previous_gke_task = current_gke_task

        main_task_group >> all_groups_end


    {% if cookiecutter.online_store_materialization %}
    default_env_vars = {'NUM_CHUNKS': 5}
    default_resources = {
        'requests': {'memory': '1Gi', 'cpu': '1'},
        'limits': {'memory': '3Gi', 'cpu': '3'}
    }
    online_store_materialization = {{cookiecutter.online_store_materialization}}
    additional_env_vars_data = online_store_materialization.get('additional_env_vars', default_env_vars)
    number_of_chunks = additional_env_vars_data.get('NUM_CHUNKS', default_env_vars.get('NUM_CHUNKS'))
    enable_auto_scale = additional_env_vars_data.get('ENABLE_AUTOSCALE', default_env_vars.get('ENABLE_AUTOSCALE'))
    container_resources_data = online_store_materialization.get('container_resources', default_resources)
    requests = container_resources_data.get('requests', default_resources['requests'])
    limits = container_resources_data.get('limits', default_resources['limits'])

    container_resources = V1ResourceRequirements(
        requests=requests,
        limits=limits
    )
    additional_env_vars = [
        V1EnvVar(name="NUM_CHUNKS", value=str(number_of_chunks)),
        V1EnvVar(name="ENABLE_AUTOSCALE", value=str(enable_auto_scale))
    ]
    online_store_materialization = OnlineStoreMaterialization(
        tables=online_store_materialization["tables"],
        additional_env_vars=additional_env_vars,
        container_resources=container_resources,
        dag=dag
    )
    {% endif %}

    all_groups_end >> {% if cookiecutter.online_store_materialization %} online_store_materialization >> {% endif %} end