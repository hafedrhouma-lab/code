# This is a template for setting up Airflow DAG configurations for training and prediction tasks.
# Configure each DAG under a unique key (like 'train', 'predict', 'train1', etc.).

example_iris_classifier_v0_train:  # Key for the training DAG. Use 'train1', 'train2', etc., for additional training DAGs if needed.
  # REQUIRED: enable_dag - Boolean indicating if the DAG should be enabled in Airflow. Default is false if not specified.
  enable_dag: true
  # REQUIRED: schedule_interval - Defines how often the DAG should run, using a cron-like string format.
  # Example: "0 3 * * Fri" means every Friday at 3:00 AM. There is no default; this must be explicitly set.
  schedule_interval: "0 3 * * Fri"

  # REQUIRED: owner - Specifies the owner of the DAG, typically the team or developer responsible.
  # Helps with accountability and maintenance. Default is "Algorithms" if not specified.
  owner: "Algorithms, Fadi Nader"

  # REQUIRED: a list of one or multiple parallel task group to run in the DAG. Each group can represent alogical case. ex: country, city, etc.
  # You can have one task group for simple use cases.
  # OR Multiple task groups for complex use cases.
  tasks_group:
    - task_group_name: "iris_classifier_egypt"
      # OPTIONAL: sql_sensors - Configuration for setting up an SQL-based sensor in Airflow.
      # This is a List of sensors that checks for certain conditions in a SQL database before running the DAG.
      # List of tasks to run in the DAG. Each task is a separate step in the DAG that start GKE pods
      sql_sensors:
        - task_id: "data_platform_fct_order_info_sensor_egypt" # Unique ID for the sensor. Must be unique within the DAG/ All tasks groups.
          # REQUIRED: enable - Boolean indicating if the sensor should be enabled. Default is false if not specified.
          enable: True
          # REQUIRED: table_name - Name of the SQL table to check.
          table_name: "fct_order_info"

          # REQUIRED: dataset_name - The dataset or database where the table is located.
          dataset_name: "data_platform"

          # REQUIRED: sensor_properties - Properties or conditions for the sensor.
          sensor_properties:
            # REQUIRED: event_date_column - Column in the SQL table that contains date information.
            event_date_column: "order_date"

            # OPTIONAL: event_date - Specific date or date calculation to check against.
            # Can use SQL expressions like "current_date()-1". Default is "current_date()-1".
            event_date: "current_date()-1"

            # OPTIONAL: lower_bound - Minimum number of rows required for the specified date.
            # If the rows are below this threshold, the DAG won't run. Default: 10
            lower_bound: 10
      # List of tasks to run in the DAG. Each task is a separate step in the DAG that start GKE pods
      gke_tasks:
        - task_id: "iris_classifier_v1_predict_egypt" # Give your task a unique name
          # OPTIONAL: node_pool - The Kubernetes node pool where the DAG's tasks should run.
          # Should match the node pool configured in your Kubernetes environment.
          # Default: "algo_generic_node_pool"
          node_pool: "ese_node_pool"

          # REQUIRED: commands - List of commands the DAG will execute, usually pointing to a script or entrypoint.
          # Example: ["python", "main.py"] runs 'main.py' with Python.
          commands:
            - "python"
            - "predict.py" # Set here your arguments for the script. ex: "main.py egy" or "train.py UAE"
          # Optional resource specifications for Pod, this will allow you to
          # set both cpu and memory limits and requirements.
          container_resources:
            requests:
              memory: "1Gi"
              cpu: "1"
            limits:
              memory: "3Gi"
              cpu: "3"
example_iris_classifier_v0_predict:  # Key for the prediction DAG. Duplicate or similar keys like 'predict1', 'predict2' are valid for additional prediction DAGs.
  enable_dag: true
  schedule_interval: "0 4 * * Mon"
  owner: "Algorithms, Fadi Nader"

  # REQUIRED: a list of one or multiple parallel task group to run in the DAG. Each group can represent alogical case. ex: country, city, etc.
  # You can have one task group for simple use cases.
  # OR Multiple task groups for complex use cases.
  tasks_group:
    - task_group_name: "iris_classifier_tasks"
      # List of tasks to run in the DAG. Each task is a separate step in the DAG that start GKE pods
      gke_tasks:
        - task_id: "iris_classifier_v1_predict" # Give your task a unique name
          # OPTIONAL: node_pool - The Kubernetes node pool where the DAG's tasks should run.
          # Should match the node pool configured in your Kubernetes environment.
          # Default: "algo_generic_node_pool"
          node_pool: "ese_node_pool"

          # REQUIRED: commands - List of commands the DAG will execute, usually pointing to a script or entrypoint.
          # Example: ["python", "main.py"] runs 'main.py' with Python.
          commands:
            - "python"
            - "predict.py" # Set here your arguments for the script. ex: "main.py egy" or "train.py UAE"

