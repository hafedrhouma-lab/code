# Configure each DAG under a unique key
# ex: example_ranker_train, example_ranker_predict, ...

# DAG #1: training DAG
example_ranker_train: # should follow the pattern: example_ranker_<script_name>
  # REQUIRED: enable_dag - Boolean indicating if the DAG should be enabled in Airflow. Default is false if not specified.
  enable_dag: True

  # REQUIRED: schedule_interval - Defines how often the DAG should run, using a cron-like string format.
  # Example: "0 3 * * Fri" means every Friday at 3:00 AM. There is no default; this must be explicitly set.
  schedule_interval: "0 3 * * Fri"

  # REQUIRED: owner - Specifies the owner of the DAG, typically the team or developer responsible.
  # Helps with accountability and maintenance. Default is "Algorithms" if not specified.
  owner: "Algorithms, Fadi Nader"

  # REQUIRED: a list of one or multiple parallel task group to run in the DAG. Each group can represent alogical case. ex: country, city, etc.
  # You can have one task group for simple use cases.
  # OR Multiple task groups for complex use cases.
  tasks_group:
    # REQUIRED: task_group_name - Name of the task group. Can be any descriptive name and should be unique within the DAG.
    - task_group_name: "task_group_1/egypt_task_group"

      # OPTIONAL: sql_sensors - Configuration for setting up an SQL-based sensor in Airflow.
      # This is a List of sensors that checks for certain conditions in a SQL database before running the DAG.
      sql_sensors:
        - task_id: "unique_sensor_id" # Unique ID for the sensor. Must be unique within the DAG.
          # task_id to follow the pattern: <table_name>_<dataset_name>_sensor

          # REQUIRED: enable - Boolean indicating if the sensor should be enabled. Default is false if not specified.
          enable: True
          # REQUIRED: table_name - Name of the SQL table to check.
          table_name: "fct_order_info"

          # REQUIRED: dataset_name - The dataset or database where the table is located.
          dataset_name: "data_platform"

          # REQUIRED: sensor_properties - Properties or conditions for the sensor.
          sensor_properties:
            # REQUIRED: event_date_column - Column in the SQL table that contains date information.
            event_date_column: "order_date"

            # OPTIONAL: event_date - Specific date or date calculation to check against.
            # Can use SQL expressions like "current_date()-1". Default is "current_date()-1".
            event_date: "current_date()-1"

            # OPTIONAL: lower_bound - Minimum number of rows required for the specified date.
            # If the rows are below this threshold, the DAG won't run. Default: 10
            lower_bound: 5

      # OPTIONAL: trigger_dag - List of DAGs to trigger after the current sql_sensors are successful.
      trigger_dags:
        - trigger_dag_id: "example_dag_id" # Required: trigger_dag_id - The DAG ID to trigger, Must be a valid DAG ID in the Airflow Algo environment.

      # List of tasks to run in the DAG. Each task is a separate step in the DAG that start GKE pods
      gke_tasks:
        - task_id: "task_id_1" # Give your task a unique name

          # OPTIONAL: node_pool - The Kubernetes node pool where the DAG's tasks should run.
          # Should match the node pool configured in your Kubernetes environment.
          # Default: "algo_generic_node_pool"
          node_pool: "ese_node_pool"

          # REQUIRED: commands - List of commands the DAG will execute, usually pointing to a script or entrypoint.
          # Example: ["python", "main.py"] runs 'main.py' with Python.
          commands:
              - "python"
              - "main.py"
              - "egy" # Set here your arguments for the script. ex: "egy" or "UAE"

  # OPTIONAL: online store materialization - List of tables to materialize in the online feature store after the DAG is successful.
  # The tables should be already registered in the feature store registry before you can materialize them to the online store.
  online_store_materialization:
    # Required: List of BigQuery tables to materialize to the online feature store
    tables:
      - "output_BQ_inference_table_id" # Set here your BigQuery table ID to be materialized to the online feature store
      - "output_BQ_inference_table_id" # Set here your BigQuery table ID to be materialized to the online feature store
    # Optional - Additional environment variables to pass to the container
    additional_env_vars:
      # Only KEY NUM_CHUNKS is allowed
      NUM_CHUNKS: 5
    # Optional - Resources to allocate to the container
    container_resources:
      requests:
        memory: "1Gi"
        cpu: "1"
      limits:
        memory: "3Gi"
        cpu: "3"

# DAG #2: Prediction DAG


# DAG ID should follow the pattern: example_ranker_<script_name>
template_model_name_predict:  # Key for the prediction DAG. Duplicate or similar keys like 'predict1', 'predict2' are valid for additional prediction DAGs.
  # REQUIRED: enable_dag - Boolean indicating if the DAG should be enabled in Airflow. Default is false if not specified.
  enable_dag: True

  schedule_interval: "0 4 * * Mon"
  owner: "Algorithms, Fadi Nader"
  # REQUIRED: a list of one or multiple parallel task group to run in the DAG. Each group can represent alogical case. ex: country, city, etc.
  # You can have one task group for simple use cases.
  # OR Multiple task groups for complex use cases.
  tasks_group:
    # REQUIRED: task_group_name - Name of the task group. Can be any descriptive name and should be unique within the DAG.
    - task_group_name: "task_group_1/egypt_task_group"
      # OPTIONAL: sql_sensors - Configuration for setting up an SQL-based sensor in Airflow.
      # This is a List of sensors that checks for certain conditions in a SQL database before running the DAG.
      sql_sensors:
        - task_id: "fct_order_info_sensor" # Unique ID for the sensor. Must be unique within the DAG.
          # REQUIRED: enable - Boolean indicating if the sensor should be enabled. Default is false if not specified.
          enable: True
          # REQUIRED: table_name - Name of the SQL table to check.
          table_name: "fct_order_info"
           # REQUIRED: dataset_name - The dataset or database where the table is located.
          dataset_name: "data_platform"
           # REQUIRED: sensor_properties - Properties or conditions for the sensor.
          sensor_properties:
             # REQUIRED: event_date_column - Column in the SQL table that contains date information.
            event_date_column: "order_date"
            # OPTIONAL: event_date - Specific date or date calculation to check against.
            # Can use SQL expressions like "current_date()-1". Default is "current_date()-1".
            event_date: "current_date()-1"
            # OPTIONAL: lower_bound - Minimum number of rows required for the specified date.
            # If the rows are below this threshold, the DAG won't run. Default: 10
            lower_bound: 10
        - task_id: "fct_order_sensor"# Unique ID for the sensor. Must be unique within the DAG.
          # REQUIRED: enable - Boolean indicating if the sensor should be enabled. Default is false if not specified.
          enable: True
          # REQUIRED: table_name - Name of the SQL table to check.
          table_name: "fct_order"
           # REQUIRED: dataset_name - The dataset or database where the table is located.
          dataset_name: "data_platform"
           # REQUIRED: sensor_properties - Properties or conditions for the sensor.
          sensor_properties:
            # REQUIRED: event_date_column - Column in the SQL table that contains date information.
            event_date_column: "order_date"
            # OPTIONAL: event_date - Specific date or date calculation to check against.
            # Can use SQL expressions like "current_date()-1". Default is "current_date()-1".
            event_date: "current_date()-1"
            # OPTIONAL: lower_bound - Minimum number of rows required for the specified date.
            # If the rows are below this threshold, the DAG won't run. Default: 10
            lower_bound: 10

      # List of tasks to run in the DAG. Each task is a separate step in the DAG that start GKE pods
      gke_tasks:
        - task_id: "task_id_1" # Give your task a unique name, ex: data_preprocessing_task
          # OPTIONAL: node_pool - The Kubernetes node pool where the DAG's tasks should run.
          # Should match the node pool configured in your Kubernetes environment.
          # Default: "algo_generic_node_pool"
          node_pool: "ese_node_pool"

          # REQUIRED: commands - List of commands the DAG will execute, usually pointing to a script or entrypoint.
          # Example: ["python", "main.py"] runs 'main.py' with Python.
          commands:
              - "python"
              - "main.py"
              - "egy" # OPTIONAL: Set here your arguments for the script. ex: "egy" or "UAE"
        - task_id: "task_id_2" # Give your task a unique name, ex: make_prediction_task
          # OPTIONAL: node_pool - The Kubernetes node pool where the DAG's tasks should run.
          # Should match the node pool configured in your Kubernetes environment.
          # Default: "algo_generic_node_pool"
          node_pool: "ese_node_pool"

          # REQUIRED: commands - List of commands the DAG will execute, usually pointing to a script or entrypoint.
          # Example: ["python", "main.py"] runs 'main.py' with Python.
          commands:
            - "python"
            - "main.py"

      # REQUIRED: task_group_name - Name of the task group. Can be any descriptive name and should be unique within the DAG.
    - task_group_name: "task_group_1/uae_task_group"
      # OPTIONAL: sql_sensors - Configuration for setting up an SQL-based sensor in Airflow.
      # This is a List of sensors that checks for certain conditions in a SQL database before running the DAG.
      sql_sensors:
        - task_id: "fct_order_info_sensor" # Unique ID for the sensor. Must be unique within the DAG.
          # REQUIRED: enable - Boolean indicating if the sensor should be enabled. Default is false if not specified.
          # task_id to follow the pattern: <table_name>_<dataset_name>_sensor

          enable: True
          # REQUIRED: table_name - Name of the SQL table to check.
          table_name: "fct_order_info"
          # REQUIRED: dataset_name - The dataset or database where the table is located.
          dataset_name: "data_platform"
          # REQUIRED: sensor_properties - Properties or conditions for the sensor.
          sensor_properties:
            # REQUIRED: event_date_column - Column in the SQL table that contains date information.
            event_date_column: "order_date"
            # OPTIONAL: event_date - Specific date or date calculation to check against.
            # Can use SQL expressions like "current_date()-1". Default is "current_date()-1".
            event_date: "current_date()-1"
            # OPTIONAL: lower_bound - Minimum number of rows required for the specified date.
            # If the rows are below this threshold, the DAG won't run. Default: 10
            lower_bound: 10
        - task_id: "fct_order_sensor"# Unique ID for the sensor. Must be unique within the DAG.
          # REQUIRED: enable - Boolean indicating if the sensor should be enabled. Default is false if not specified.
          enable: True
          # REQUIRED: table_name - Name of the SQL table to check.
          table_name: "fct_order"
          # REQUIRED: dataset_name - The dataset or database where the table is located.
          dataset_name: "data_platform"
          # REQUIRED: sensor_properties - Properties or conditions for the sensor.
          sensor_properties:
            # REQUIRED: event_date_column - Column in the SQL table that contains date information.
            event_date_column: "order_date"
            # OPTIONAL: event_date - Specific date or date calculation to check against.
            # Can use SQL expressions like "current_date()-1". Default is "current_date()-1".
            event_date: "current_date()-1"
            # OPTIONAL: lower_bound - Minimum number of rows required for the specified date.
            # If the rows are below this threshold, the DAG won't run. Default: 10
            lower_bound: 10

      # List of tasks to run in the DAG. Each task is a separate step in the DAG that start GKE pods
      gke_tasks:
        - task_id: "task_id_1" # Give your task a unique name, ex: data_preprocessing_task

          # OPTIONAL: node_pool - The Kubernetes node pool where the DAG's tasks should run.
          # Should match the node pool configured in your Kubernetes environment.
          # Default: "algo_generic_node_pool"
          node_pool: "ese_node_pool"

          # REQUIRED: commands - List of commands the DAG will execute, usually pointing to a script or entrypoint.
          # Example: ["python", "main.py"] runs 'main.py' with Python.
          commands:
            - "python"
            - "main.py"
            - "uae"  # OPTIONAL: Set here your arguments for the script. ex: "main.py egy" or "train.py UAE"
        - task_id: "task_id_2" # Give your task a unique name, ex: make_prediction_task
          # OPTIONAL: node_pool - The Kubernetes node pool where the DAG's tasks should run.
          # Should match the node pool configured in your Kubernetes environment.
          # Default: "algo_generic_node_pool"
          node_pool: "ese_node_pool"

          # REQUIRED: commands - List of commands the DAG will execute, usually pointing to a script or entrypoint.
          # Example: ["python", "main.py"] runs 'main.py' with Python.
          commands:
            - "python"
            - "main.py"
            - "uae" # OPTIONAL: Set here your arguments for the script. ex: "main.py egy" or "train.py UAE"
  # OPTIONAL: online store materialization - List of tables to materialize in the online feature store after the DAG is successful.
  # The tables should be already registered in the feature store registry before you can materialize them to the online store.
  online_store_materialization:
    # Required: List of BigQuery tables to materialize to the online feature store
    tables:
      - "output_BQ_inference_table_id" # Set here your BigQuery table ID to be materialized to the online feature store
      - "output_BQ_inference_table_id" # Set here your BigQuery table ID to be materialized to the online feature store
    # Optional - Additional environment variables to pass to the container
    additional_env_vars:
      # Only KEY NUM_CHUNKS is allowed
      NUM_CHUNKS: 5
    # Optional - Resources to allocate to the container
    container_resources:
      requests:
        memory: "1Gi"
        cpu: "1"
      limits:
        memory: "3Gi"
        cpu: "3"